{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nferx_py v1.7.6 is currently installed. Please upgrade to the latest version v2.0.0. Installation link: https://github.com/lumenbiomics/nferx_py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-08 09:41:06,968 : INFO : URL: https://preview.nferx.com/api/is_authenticated, process-time: 0.5146870613098145\n",
      "2020-06-08 09:41:06,969 : INFO : Authentication successful - nferX is online\n",
      "2020-06-08 09:41:07,403 : INFO : URL: https://preview.nferx.com/api/get_data_versions, process-time: 0.432873010635376\n",
      "2020-06-08 09:41:07,404 : INFO : Data version \"202003\" has been set as default one\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import ast\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "from Bio import SeqIO\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "import functools\n",
    "import operator as op\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import nferx_py.fn as nf\n",
    "\n",
    "product = lambda iterable: functools.reduce(op.mul, iterable, 1)\n",
    "nf.authenticate(\"nikhilkayal@nference.net\", \"063a1c676867563c7898102c1dfd9b0e\", \"preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing human & corona proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronavirus = \"/Users/nikhil/Downloads/37975558536-AllCoronaviridae_ProteinFastaResults.fasta\"\n",
    "human = \"/Users/nikhil/nference/repositories/nfer/nferxapps/bio2vec_api/common/resources/uniprot-proteome-tabs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coronavirus_fasta(file):\n",
    "    print(\"Reading: \", file)\n",
    "    corona_family = {}\n",
    "    for record in SeqIO.parse(file, \"fasta\"):\n",
    "        corona_family[str(record.description)] = str(record.seq)\n",
    "    corona_id_to_seq = {}\n",
    "    corona_id_to_organism = {}\n",
    "    corona_id_to_protein = {}\n",
    "    for key, value in corona_family.items():\n",
    "        corona_id = key.split(\"|\")[1].split(\":\")[1]\n",
    "        corona_organism = key.split(\"|\")[3].split(\":\")[1]\n",
    "        corona_id_to_seq[corona_id] = value\n",
    "        corona_id_to_organism[corona_id] = corona_organism\n",
    "        if len(key.split(\"|\")) > 5:\n",
    "            corona_protein = key.split(\"|\")[5].split(\":\")[1]\n",
    "        corona_id_to_protein[corona_id] = corona_protein\n",
    "    organism_to_keys = defaultdict(list)\n",
    "    for key, value in sorted(corona_id_to_organism.items()):\n",
    "        organism_to_keys[value].append(key)\n",
    "    organism_to_keys = dict(organism_to_keys)\n",
    "    return corona_id_to_seq, corona_id_to_protein, organism_to_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading:  /Users/nikhil/Downloads/37975558536-AllCoronaviridae_ProteinFastaResults.fasta\n"
     ]
    }
   ],
   "source": [
    "corona_id_to_seq, corona_id_to_protein, organism_to_keys = parse_coronavirus_fasta(coronavirus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_uniprot_exome(uniprot_exome_file):\n",
    "    fd = open(uniprot_exome_file, 'rU')\n",
    "    # delim = '\\t' if (uniprot_exome_file.endswith('.tsv')) else ','\n",
    "    delim = '\\t'\n",
    "    csv_file = csv.reader(fd, delimiter=delim)\n",
    "    next(csv_file)  # skip header\n",
    "\n",
    "    print(\"Reading: \", uniprot_exome_file)\n",
    "\n",
    "    seq_dict = dict()\n",
    "    protein_dict = dict()\n",
    "    uniprot_dict = dict()\n",
    "    uniprot2protein = dict()\n",
    "    uniprot2gene = dict()\n",
    "\n",
    "    for row in csv_file:\n",
    "        uniprot_id = row[0]\n",
    "        protein = row[1]\n",
    "        gene = row[2]\n",
    "        seq = row[3]\n",
    "\n",
    "        if len(gene) == 0:\n",
    "            if len(protein) > 0:\n",
    "                gene = protein\n",
    "            else:\n",
    "                gene = ['uniprot_' + uniprot_id]\n",
    "                pass\n",
    "        gene_str = gene.split()[0]\n",
    "        gene_str = gene_str.upper()\n",
    "        seq_dict[gene_str] = seq\n",
    "        protein_dict[gene_str] = protein\n",
    "        uniprot_dict[uniprot_id] = seq\n",
    "        uniprot2protein[uniprot_id] = protein\n",
    "        uniprot2gene[uniprot_id] = gene_str\n",
    "\n",
    "    return seq_dict, protein_dict, uniprot_dict, uniprot2protein, uniprot2gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading:  /Users/nikhil/nference/repositories/nfer/nferxapps/bio2vec_api/common/resources/uniprot-proteome-tabs.txt\n"
     ]
    }
   ],
   "source": [
    "seq_dict, protein_dict, uniprot_dict, uniprot2protein, uniprot2gene = read_uniprot_exome(human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_nmers(sequence_dict, n):\n",
    "#     counts = defaultdict(int)\n",
    "#     for sequence in sequence_dict.values():\n",
    "#         for start_ix in range(len(sequence) - n + 1):\n",
    "#             counts[sequence[start_ix:start_ix + n]] += 1\n",
    "#     series = pd.Series(counts)\n",
    "#     return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_probs(sequence_dict, n):\n",
    "#     symbol_counts = count_nmers(sequence_dict, 1)\n",
    "#     symbol_freqs = (symbol_counts / symbol_counts.sum()).to_dict()\n",
    "#     counts = count_nmers(sequence_dict, n)\n",
    "#     probs = {nmer: product(symbol_freqs[symbol] for symbol in nmer) for nmer in list(counts.index)}\n",
    "#     series = pd.Series(probs)\n",
    "#     return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def computer_nmer_suprise(sequence_dict, n, db):\n",
    "# #     counts = self.count_nmers(n)\n",
    "#     counts = pd.read_csv(db + \"_{0}.csv\".format(n), index_col=0, header=None, keep_default_na=False)[1]\n",
    "# #     probs = self.compute_probs(n)\n",
    "#     probs = pd.read_csv(db + \"_prob_{0}.csv\".format(n), index_col=0, header=None, keep_default_na=False)[1]\n",
    "#     total_length_n_sequences = sum(max(0, len(seq) - n + 1) for seq in sequence_dict.values())\n",
    "#     expected = probs * total_length_n_sequences\n",
    "#     surprise_ratios = counts / expected\n",
    "#     log2_surprise_ratio = np.log2(surprise_ratios)\n",
    "#     variances = probs * (1 - probs) * total_length_n_sequences\n",
    "#     stds = np.sqrt(variances)\n",
    "#     stds_from_mean = (counts - expected) / stds\n",
    "#     percentiles = np.array([binom.cdf(count, n=total_length_n_sequences, p=prob)\n",
    "#                                     for count, prob in zip(counts, probs)])\n",
    "#     survivals = np.array([binom.sf(count, n=total_length_n_sequences, p=prob)\n",
    "#                                   for count, prob in zip(counts, probs)])  # 1 - percentiles, but more accurate\n",
    "#     uniqueness_percentile_score = -np.log10(percentiles * 100)\n",
    "#     uniqueness_percentile_score[np.isinf(uniqueness_percentile_score)] = 500\n",
    "#     conservation_percentile_score = -np.log10(survivals * 100)\n",
    "#     conservation_percentile_score[np.isinf(conservation_percentile_score)] = 500\n",
    "#     df = pd.DataFrame(dict(\n",
    "#                     count=counts,\n",
    "#                     probability=probs,\n",
    "#                     expected=expected,\n",
    "#                     surprise_ratio=surprise_ratios,\n",
    "#                     log2_surprise_ratio=log2_surprise_ratio,\n",
    "#                     std=stds,\n",
    "#                     stds_from_mean=stds_from_mean,\n",
    "#                     percentile=percentiles,\n",
    "#                     uniqueness_percentile_score=uniqueness_percentile_score,\n",
    "#                     conservation_percentile_score=conservation_percentile_score\n",
    "#                 ))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniprot_8_surprise = computer_nmer_suprise(uniprot_dict, 8, \"uniprot\")\n",
    "# uniprot_8_surprise.to_csv(\"uniprot_8_surprise.csv\")\n",
    "\n",
    "# uniprot_9_surprise = computer_nmer_suprise(uniprot_dict, 9, \"uniprot\")\n",
    "# uniprot_9_surprise.to_csv(\"uniprot_9_surprise.csv\")\n",
    "\n",
    "# uniprot_10_surprise = computer_nmer_suprise(uniprot_dict, 10, \"uniprot\")\n",
    "# uniprot_10_surprise.to_csv(\"uniprot_10_surprise.csv\")\n",
    "\n",
    "# viral_8_surprise = computer_nmer_suprise(corona_id_to_seq, 8, \"viral\")\n",
    "# viral_8_surprise.to_csv(\"coronavirus/corona_8_surprise.csv\")\n",
    "\n",
    "# viral_9_surprise = computer_nmer_suprise(corona_id_to_seq, 9, \"viral\")\n",
    "# viral_9_surprise.to_csv(\"coronavirus/corona_9_surprise.csv\")\n",
    "\n",
    "# viral_10_surprise = computer_nmer_suprise(corona_id_to_seq, 10, \"viral\")\n",
    "# viral_10_surprise.to_csv(\"coronavirus/corona_10_surprise.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find longest matching string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_substring(s1, s2):\n",
    "    t = [[0]*(1+len(s2)) for i in range(1+len(s1))]\n",
    "    l, xl = 0, 0\n",
    "    for x in range(1,1+len(s1)):\n",
    "        for y in range(1,1+len(s2)):\n",
    "            if s1[x-1] == s2[y-1]:\n",
    "                t[x][y] = t[x-1][y-1] + 1\n",
    "                if t[x][y]>l:\n",
    "                    l = t[x][y]\n",
    "                    xl  = x\n",
    "            else:\n",
    "                t[x][y] = 0\n",
    "    return s1[xl-l: xl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_viral_species(key_to_find):\n",
    "    for key, value in organism_to_keys.items():\n",
    "        if key_to_find in value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT05254.1\n",
      "ALT05263.1\n",
      "ALT05264.1\n",
      "ALT05255.1\n",
      "ALT05256.1\n",
      "ALT05257.1\n",
      "ALT05258.1\n",
      "ALT05259.1\n",
      "ALT05260.1\n",
      "ALT05261.1\n",
      "ALT05262.1\n",
      "AOC31971.1\n",
      "AOC31972.1\n",
      "AOC31973.1\n",
      "AOC31974.1\n",
      "AOC31975.1\n",
      "AOC31976.1\n",
      "AOC31977.1\n",
      "AOC31978.1\n",
      "AOC31982.1\n",
      "AOC31979.1\n",
      "AOC31980.1\n",
      "AOC31981.1\n",
      "AZF86122.1\n",
      "AZF86119.1\n",
      "AZF86118.1\n",
      "AZF86120.1\n",
      "AZF86121.1\n",
      "AZF86134.1\n",
      "AZF86130.1\n",
      "AZF86131.1\n",
      "AZF86132.1\n",
      "AZF86133.1\n",
      "AZF86128.1\n",
      "AZF86127.1\n",
      "AZF86126.1\n",
      "AZF86125.1\n",
      "AZF86124.1\n",
      "YP_009755891.1\n",
      "YP_009755890.1\n",
      "YP_009755894.1\n",
      "YP_009755892.1\n",
      "YP_009755893.1\n",
      "QBG64639.1\n",
      "VIPR_ALG1_AVY53335_1_11507_11842.1\n",
      "VIPR_ALG1_AVY53335_1_8966_9787.1\n",
      "VIPR_ALG1_AVY53335_1_11873_12229.1\n",
      "VIPR_ALG1_AVY53335_1_9485_10495.1\n",
      "VIPR_ALG1_AVY53335_1_9815_10099.1\n",
      "AVY53336.1\n",
      "AVY53337.1\n",
      "AVY53338.1\n",
      "AVY53339.1\n",
      "AVY53340.1\n",
      "AVY53341.1\n",
      "AVY53342.1\n",
      "VIPR_ALG1_AVY53334_1_12286_13332.1\n",
      "VIPR_ALG1_AVY53334_1_15838_16275.1\n",
      "VIPR_ALG1_AVY53334_1_16084_16803.1\n",
      "VIPR_ALG1_AVY53334_1_16312_16773.1\n",
      "VIPR_ALG1_AVY53334_1_16837_18579.1\n",
      "VIPR_ALG1_AVY53334_1_19396_20289.1\n",
      "VIPR_ALG1_AVY53335_1_10673_10921.1\n",
      "VIPR_ALG1_AVY53335_1_10922_11506.1\n",
      "VIPR_ALG1_AVY53335_1_275_601.1\n",
      "VIPR_ALG1_AVY53335_1_3446_4069.1\n"
     ]
    }
   ],
   "source": [
    "temp_l = []\n",
    "for corona_id, corona_seq in corona_id_to_seq.items():\n",
    "    print(corona_id)\n",
    "    for human_id, human_seq in uniprot_dict.items():\n",
    "        match = longest_substring(corona_seq, human_seq)\n",
    "        if match:\n",
    "            temp_dict = dict()\n",
    "            temp_dict[\"peptide\"] = match\n",
    "            temp_dict[\"peptide_length\"] = len(match)\n",
    "            temp_dict[\"human_protein\"] = uniprot2gene[human_id]\n",
    "            temp_dict[\"human_protein_annotation\"] = human_id\n",
    "            temp_dict[\"viral_species\"] = find_viral_species(corona_id)\n",
    "            temp_dict[\"viral_protein\"] = corona_id_to_protein[corona_id]\n",
    "            temp_dict[\"viral_protein_annotation\"] = corona_id\n",
    "            temp_dict[\"human_protein_peptide_start\"] = uniprot_dict[human_id].find(match)\n",
    "            temp_dict[\"human_protein_peptide_end\"] = temp_dict[\"human_protein_peptide_start\"] + temp_dict[\"peptide_length\"] - 1\n",
    "            temp_dict[\"viral_protein_peptide_start\"] = corona_id_to_seq[corona_id].find(match)\n",
    "            temp_dict[\"viral_protein_peptide_end\"] = temp_dict[\"viral_protein_peptide_start\"] + temp_dict[\"peptide_length\"] - 1\n",
    "            temp_l.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156794"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
